{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "hC-f8hZeg3Wc",
        "outputId": "fcce43d8-f4ab-49e1-ab8d-7d2309b993af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting scraper with the specified settings...\n",
            "Using cutoff date: 2025-09-27 00:00:00\n",
            "\n",
            "--- Scraping Danbooru ---\n",
            "Danbooru page 1 processed.\n",
            "Danbooru page 2 processed.\n",
            "Danbooru page 3 processed.\n",
            "Danbooru page 4 processed.\n",
            "Danbooru page 5 processed.\n",
            "Danbooru page 6 processed.\n",
            "Danbooru page 7 processed.\n",
            "Danbooru page 8 processed.\n",
            "Danbooru page 9 processed.\n",
            "Danbooru page 10 processed.\n",
            "Danbooru page 11 processed.\n",
            "Danbooru page 12 processed.\n",
            "Danbooru page 13 processed.\n",
            "Danbooru page 14 processed.\n",
            "Danbooru page 15 processed.\n",
            "Danbooru page 16 processed.\n",
            "Danbooru page 17 processed.\n",
            "Danbooru page 18 processed.\n",
            "Danbooru page 19 processed.\n",
            "Danbooru page 20 processed.\n",
            "Danbooru page 21 processed.\n",
            "Danbooru page 22 processed.\n",
            "Danbooru page 23 processed.\n",
            "Danbooru page 24 processed.\n",
            "Danbooru page 25 processed.\n",
            "Danbooru page 26 processed.\n",
            "Danbooru page 27 processed.\n",
            "Danbooru page 28 processed.\n",
            "Danbooru page 29 processed.\n",
            "Danbooru page 30 processed.\n",
            "Danbooru page 31 processed.\n",
            "Danbooru page 32 processed.\n",
            "Danbooru page 33 processed.\n",
            "Danbooru page 34 processed.\n",
            "Danbooru page 35 processed.\n",
            "Danbooru page 36 processed.\n",
            "Danbooru page 37 processed.\n",
            "Danbooru page 38 processed.\n",
            "Danbooru page 39 processed.\n",
            "Danbooru page 40 processed.\n",
            "Danbooru page 41 processed.\n",
            "Danbooru page 42 processed.\n",
            "Danbooru page 43 processed.\n",
            "Danbooru page 44 processed.\n",
            "Danbooru page 45 processed.\n",
            "Danbooru page 46 processed.\n",
            "Danbooru page 47 processed.\n",
            "Danbooru page 48 processed.\n",
            "Danbooru page 49 processed.\n",
            "Danbooru page 50 processed.\n",
            "Danbooru page 51 processed.\n",
            "Danbooru page 52 processed.\n",
            "Danbooru page 53 processed.\n",
            "Danbooru page 54 processed.\n",
            "Danbooru page 55 processed.\n",
            "Danbooru page 56 processed.\n",
            "Danbooru page 57 processed.\n",
            "Danbooru page 58 processed.\n",
            "Danbooru page 59 processed.\n",
            "Danbooru page 60 processed.\n",
            "Danbooru page 61 processed.\n",
            "Danbooru page 62 processed.\n",
            "Danbooru page 63 processed.\n",
            "Danbooru page 64 processed.\n",
            "Danbooru page 65 processed.\n",
            "Danbooru page 66 processed.\n",
            "Danbooru page 67 processed.\n"
          ]
        }
      ],
      "source": [
        "#@title Tag Scraper\n",
        "#@markdown ### ‚öôÔ∏è Core Settings\n",
        "#@markdown Enter the desired settings below. The script will use these values when you run the cell.\n",
        "Output_Filename = \"danbooru2026.csv\" #@param {type:\"string\"}\n",
        "Minimum_Tag_Count = 20 #@param {type:\"integer\"}\n",
        "Boards_to_Scrape = \"Danbooru only\" #@param [\"Danbooru only\", \"e621 only\", \"Both (merged)\"]\n",
        "Cutoff_Date = \"2025-09-27\" #@param {type:\"date\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìù Formatting Options\n",
        "Replace_Underscore_with_Dash = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üö´ Category Exclusions\n",
        "#@markdown Check the boxes for any tag categories you wish to exclude from the final list.\n",
        "Exclude_General = False #@param {type:\"boolean\"}\n",
        "Exclude_Artist = False #@param {type:\"boolean\"}\n",
        "Exclude_Copyright = False #@param {type:\"boolean\"}\n",
        "Exclude_Character = False #@param {type:\"boolean\"}\n",
        "Exclude_Post = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "import collections\n",
        "import csv\n",
        "import time\n",
        "import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# Step 2: Process the inputs from the Colab form\n",
        "csv_filename = Output_Filename\n",
        "minimum_count = Minimum_Tag_Count\n",
        "dashes = 'y' if Replace_Underscore_with_Dash else 'n'\n",
        "date = Cutoff_Date\n",
        "\n",
        "if Boards_to_Scrape == \"Danbooru only\":\n",
        "    boards = 'd'\n",
        "elif Boards_to_Scrape == \"e621 only\":\n",
        "    boards = 'e'\n",
        "else:\n",
        "    boards = 'de'\n",
        "\n",
        "exclude = []\n",
        "if Exclude_General: exclude.append('general')\n",
        "if Exclude_Artist: exclude.append('artist')\n",
        "if Exclude_Copyright: exclude.append('copyright')\n",
        "if Exclude_Character: exclude.append('character')\n",
        "if Exclude_Post: exclude.append('post')\n",
        "\n",
        "\n",
        "# Step 3: The Scraper Logic (adapted from your script)\n",
        "print(\"üöÄ Starting scraper with the specified settings...\")\n",
        "\n",
        "class Complete(Exception): pass\n",
        "\n",
        "try:\n",
        "    max_date = datetime.datetime.strptime(date.strip()[:10], \"%Y-%m-%d\")\n",
        "    print(f\"Using cutoff date: {max_date}\")\n",
        "except:\n",
        "    max_date = datetime.datetime.now()\n",
        "    print(f\"Using today's date: {max_date}\")\n",
        "\n",
        "excluded = \"\"\n",
        "excluded += \"0\" if \"general\" in exclude else \"\"\n",
        "excluded += \"1\" if \"artist\" in exclude else \"\"\n",
        "excluded += \"3\" if \"copyright\" in exclude else \"\"\n",
        "excluded += \"4\" if \"character\" in exclude else \"\"\n",
        "excluded += \"5\" if \"post\" in exclude else \"\"\n",
        "\n",
        "kaomojis = [\n",
        "    \"0_0\", \"(o)_(o)\", \"+_+\", \"+_-\", \"._.\", \"<o>_<o>\", \"<|>_<|>\", \"=_=\", \">_<\",\n",
        "    \"3_3\", \"6_9\", \">_o\", \"@_@\", \"^_^\", \"o_o\", \"u_u\", \"x_x\", \"|_|\", \"||_||\",\n",
        "]\n",
        "\n",
        "if not '.csv' in csv_filename:\n",
        "    csv_filename += '.csv'\n",
        "\n",
        "temp_csv_filename = csv_filename\n",
        "if dashes == 'y':\n",
        "    temp_csv_filename += '-temp'\n",
        "\n",
        "base_url = 'https://danbooru.donmai.us/tags.json?limit=1000&search[hide_empty]=yes&search[is_deprecated]=no&search[order]=count'\n",
        "alias_url = 'https://danbooru.donmai.us/tag_aliases.json?commit=Search&limit=1000&search[order]=tag_count'\n",
        "e6_base_url = 'https://e621.net/tags.json?limit=1000&search[hide_empty]=yes&search[is_deprecated]=no&search[order]=count'\n",
        "e6_alias_url = 'https://e621.net/tag_aliases.json?commit=Search&limit=1000&search[order]=tag_count'\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"Colab-Tag-Scraper/1.0\"})\n",
        "\n",
        "def backdate(tags, aliases, date):\n",
        "    print(f\"Clearing older aliases...\")\n",
        "    filtered_aliases = {}\n",
        "    for key in aliases:\n",
        "        kept = []\n",
        "        for item in aliases[key]:\n",
        "            entry_date = datetime.datetime.strptime(item[1][:10], \"%Y-%m-%d\")\n",
        "            if entry_date <= date:\n",
        "                kept += [item[0]]\n",
        "        filtered_aliases[key] = kept\n",
        "\n",
        "    for key in list(tags.keys()):\n",
        "        if datetime.datetime.strptime(tags[key][2][:10], \"%Y-%m-%d\") > date:\n",
        "            try:\n",
        "                new_key = filtered_aliases[key].pop(0)\n",
        "                value = tags.pop(key)\n",
        "                tags[new_key] = value\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    for key in filtered_aliases:\n",
        "        try:\n",
        "            alias_string = \",\".join(filtered_aliases[key])\n",
        "            tags[key] += [alias_string]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "def get_aliases(url, type):\n",
        "    try:\n",
        "        aliases = collections.defaultdict(list)\n",
        "        for page in range(1, 1001):\n",
        "            full_url = f'{url}&page={page}'\n",
        "            while True:\n",
        "                try:\n",
        "                    response = session.get(full_url)\n",
        "                    if response.status_code == 200:\n",
        "                        break\n",
        "                    else:\n",
        "                        print(f\"Couldn't reach server, Status: {response.status_code}. Retrying in 5s\")\n",
        "                        time.sleep(5)\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"Request failed: {e}. Retrying in 10s\")\n",
        "                    time.sleep(10)\n",
        "            data = response.json()\n",
        "            if not data:\n",
        "                print(f'No more alias data found at page {page}.')\n",
        "                break\n",
        "            for item in data:\n",
        "                if type == \"e\":\n",
        "                    if int(item['post_count']) < int(minimum_count):\n",
        "                        raise Complete\n",
        "                aliases[item['consequent_name']] += [[item['antecedent_name'], item['created_at']]]\n",
        "            print(f'Page {page} aliases processed.', flush=True)\n",
        "            time.sleep(0.1)\n",
        "    except(Complete):\n",
        "        print(\"Reached the post threshold for aliases.\")\n",
        "    return aliases\n",
        "\n",
        "dan_tags = {}\n",
        "if \"d\" in boards:\n",
        "    print(\"\\n--- Scraping Danbooru ---\")\n",
        "    try:\n",
        "        for page in range(1, 1001):\n",
        "            full_url = f'{base_url}&page={page}'\n",
        "            response = session.get(full_url)\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Failed to fetch Danbooru page {page}. Status: {response.status_code}\")\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "            data = response.json()\n",
        "            if not data:\n",
        "                print(f'No more Danbooru data found at page {page}.')\n",
        "                break\n",
        "            for item in data:\n",
        "                if int(item['post_count']) < int(minimum_count):\n",
        "                    raise Complete\n",
        "                if not str(item['category']) in excluded:\n",
        "                    dan_tags[item['name']] = [item['category'], item['post_count'], item['created_at']]\n",
        "            print(f'Danbooru page {page} processed.', flush=True)\n",
        "            time.sleep(0.1)\n",
        "    except(Complete):\n",
        "        print(f'All Danbooru tags with {minimum_count} posts or more have been scraped.')\n",
        "\n",
        "    dan_aliases = get_aliases(alias_url, \"d\")\n",
        "    backdate(dan_tags, dan_aliases, max_date)\n",
        "\n",
        "e6_tags = {}\n",
        "if \"e\" in boards:\n",
        "    print(\"\\n--- Scraping e621 ---\")\n",
        "    try:\n",
        "        for page in range(1, 1001):\n",
        "            full_url = f'{e6_base_url}&page={page}'\n",
        "            response = session.get(full_url)\n",
        "            if response.status_code != 200:\n",
        "                print(f'Failed to fetch e621 page {page}. Status: {response.status_code}')\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "            data = response.json()\n",
        "            if not data:\n",
        "                print(f'No more e621 data found at page {page}.')\n",
        "                break\n",
        "            for item in data:\n",
        "                if int(item['post_count']) < int(minimum_count):\n",
        "                    raise Complete\n",
        "                if not str(item['category']) in excluded:\n",
        "                    e6_tags[item['name']] = [item['category'], item['post_count'], item['created_at']]\n",
        "            print(f'e621 page {page} processed.', flush=True)\n",
        "            time.sleep(1) # e621 requires a 1-second delay\n",
        "    except Complete:\n",
        "        print(f'All e621 tags with {minimum_count} posts or more have been scraped.')\n",
        "\n",
        "if (\"d\" in boards) and (\"e\" in boards):\n",
        "    print(\"\\nMerging Danbooru and e621 lists...\")\n",
        "    for tag, data in dan_tags.items():\n",
        "        if tag in e6_tags:\n",
        "            e6_tags[tag][1] += data[1] # Combine post counts\n",
        "        else:\n",
        "            e6_tags[tag] = data # Add Danbooru tag if not in e6\n",
        "    full_tags = e6_tags\n",
        "elif \"d\" in boards:\n",
        "    full_tags = dan_tags\n",
        "else:\n",
        "    full_tags = e6_tags\n",
        "\n",
        "print(f\"\\nWriting data to {temp_csv_filename}...\")\n",
        "with open(temp_csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['tag', 'category', 'post_count', 'aliases']) # Write header\n",
        "    for key, value in full_tags.items():\n",
        "        try:\n",
        "            writer.writerow([key, value[0], value[1], value[3]])\n",
        "        except IndexError:\n",
        "            writer.writerow([key, value[0], value[1], ''])\n",
        "\n",
        "final_filename = temp_csv_filename\n",
        "if dashes == 'y':\n",
        "    final_filename = csv_filename\n",
        "    print(f\"Replacing '_' with '-' and saving to {final_filename}...\")\n",
        "    with open(temp_csv_filename, 'r', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        with open(final_filename, 'w', encoding='utf-8', newline='') as outfile:\n",
        "            writer = csv.writer(outfile)\n",
        "            next(reader) # Skip header in read file\n",
        "            writer.writerow(['tag', 'category', 'post_count', 'aliases']) # Write header to new file\n",
        "            for row in reader:\n",
        "                if not row[0] in kaomojis:\n",
        "                    row[0] = row[0].replace(\"_\", \"-\")\n",
        "                    if len(row) > 3: # Check if aliases column exists\n",
        "                        row[3] = row[3].replace(\"_\", \"-\")\n",
        "                writer.writerow(row)\n",
        "    os.remove(temp_csv_filename)\n",
        "\n",
        "print(f\"\\n‚úÖ Success! Data has been written to {final_filename}\")\n",
        "print(\"Starting download...\")\n",
        "files.download(final_filename)"
      ]
    }
  ]
}